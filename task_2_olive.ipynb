{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 \n",
    "\n",
    "\n",
    "### Task 2.1\n",
    "Get list of US politicians with political affiliation. \n",
    "\n",
    "2 sources:\n",
    "- https://github.com/casmlab/politicians-tweets \n",
    "- https://www.congress.gov/members?q={%22congress%22:[%22110%22,%22111%22,%22112%22,%22113%22,%22114%22,%22115%22,%22116%22,117]}\n",
    "\n",
    "Take 1st list, keep politicians whose affiliation is known. Then merge with congress list to be sure we have a good dataset.\n",
    "List of politicians is in `data/ressources/politicians.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import json\n",
    "import time\n",
    "import csv\n",
    "import bz2\n",
    "\n",
    "# from tqdm.contrib.concurrent import process_map\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from multiprocesspandas import applyparallel\n",
    "import numpy as np\n",
    "\n",
    "# tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config part\n",
    "DATA_PATH = \"data\"\n",
    "PKL_PATH = os.path.join(DATA_PATH, \"pkl\")\n",
    "RESOURCES_PATH = os.path.join(DATA_PATH, \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df(\n",
    "    file_name: str, mode: str = \"pandas\", save: bool = True, chunksize: int = 1_000_000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a dataset in DataFrame from a .json.bz2 archive.\n",
    "\n",
    "    file_name: str\n",
    "        Name of .json.bz2 archive to load from `DATA_PATH`.\n",
    "\n",
    "    mode: str = \"pandas\" | \"bz2\"\n",
    "        Either use pandas read_json function or homemade bz2 function. This is usually faster (but makes my computer crash for some reason).\n",
    "\n",
    "    save: bool\n",
    "        Save the dataframe as a pickle file in `PKL_PATH`.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(DATA_PATH, file_name)\n",
    "    pkl_path = os.path.join(PKL_PATH, f\"{file_name}.pkl\")\n",
    "\n",
    "    if mode == \"bz2\":\n",
    "        keys = [\"quoteID\", \"quotation\", \"speaker\", \"date\", \"numOccurrences\", \"phase\"]\n",
    "\n",
    "        with bz2.open(file_path, \"rb\") as quote_file:\n",
    "            df = pd.DataFrame(\n",
    "                [\n",
    "                    dict(zip(keys, map(json.loads(instance).get, keys)))\n",
    "                    for instance in tqdm(quote_file)\n",
    "                ]\n",
    "            )\n",
    "    else:\n",
    "        df_lst = []\n",
    "        with pd.read_json(file_path, lines=True, chunksize=chunksize) as df_reader:\n",
    "            for i, chunk in enumerate(df_reader):\n",
    "                df_lst.append(chunk)\n",
    "\n",
    "        df = pd.concat(df_lst)\n",
    "\n",
    "    if save and not os.path.exists(pkl_path):\n",
    "        file_name = file_name.strip(\".json.bz2\")\n",
    "        df.to_pickle(os.path.join(PKL_PATH, pkl_path))\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(file_name: str, pol_lst: list) -> None:\n",
    "    \"\"\"\n",
    "    Write list to csv.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_path = os.path.join(\"data\", \"resources\", file_name)\n",
    "\n",
    "    with open(csv_path, \"w\") as f:\n",
    "        writer = csv.writer(f, delimiter=\" \")\n",
    "        writer.writerow([\"Name\", \"Party\"])\n",
    "\n",
    "        for member in pol_lst:\n",
    "            writer.writerow([el for el in member])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Github dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"politicians_github.json\"\n",
    "file_path = os.path.join(\"data\", \"resources\", file_name)\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    json = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'id_str', 'screen_name', 'confirmed_account_type', 'state', 'twitter_name', 'real_name', 'bioguide', 'office_holder', 'party', 'district', 'level', 'woman', 'birthday', 'last_updated'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9979/9979 [00:00<00:00, 311395.34it/s]\n"
     ]
    }
   ],
   "source": [
    "# Only keep politicians with political affiliation\n",
    "politicians = []\n",
    "\n",
    "for i in tqdm(range(1, len(json[\"id\"]))):\n",
    "    i = str(i)\n",
    "    affiliation = json[\"party\"][i]\n",
    "    screen_name = json[\"screen_name\"][i]\n",
    "    elected = json[\"office_holder\"][i] is not None\n",
    "\n",
    "    if affiliation is not None and affiliation in (\"Republican\", \"Democratic\"):\n",
    "        politicians.append((json[\"real_name\"][i], affiliation, elected))\n",
    "    elif screen_name == \"realdonaldtrump\":\n",
    "        politicians.append((\"Donald Trump\", \"Republican\", True))\n",
    "    elif screen_name == \"barackobama\":\n",
    "        politicians.append((\"Barack Obama\", \"Democratic\", True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1107"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count how many politicians are \"elected\" (-> congress members)\n",
    "sum(pol[-1] for pol in politicians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All politicians are in Congress!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(politicians)=1107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mark Green', 'Republican', True),\n",
       " ('Pete Stauber', 'Republican', True),\n",
       " ('Derek Kilmer', 'Democratic', True),\n",
       " ('Andy Harris', 'Republican', True),\n",
       " ('Donald Payne', 'Democratic', True),\n",
       " ('A. Ferguson', 'Republican', True),\n",
       " ('Richard Hudson', 'Republican', True),\n",
       " ('Edward Markey', 'Democratic', True),\n",
       " ('Bobby Rush', 'Democratic', True),\n",
       " ('Gregory Meeks', 'Democratic', True)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(f\"{len(politicians)=}\") \n",
    "politicians[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "to_csv(\"politicians_github.csv\", politicians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### US Congress dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.congress.gov/members?q={\"congress\":[\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",117]}&pageSize=250'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip and clean name.\n",
    "    \"Senator Cruz, Ted\" -> \"Ted Cruz\"\n",
    "    \"\"\"\n",
    "\n",
    "    for element in (\"Representative\", \"Senator\"):\n",
    "        name = name.strip(element)\n",
    "\n",
    "    name = \" \".join(name.split(\",\")[::-1])\n",
    "    name = name.strip()\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:20<00:00,  4.07s/it]\n"
     ]
    }
   ],
   "source": [
    "congress_members = []\n",
    "\n",
    "# Download each congress page\n",
    "with requests.Session() as s:\n",
    "    for page_number in tqdm(range(1, 6)):\n",
    "        r  = s.get(URL, params={\"page\": page_number})\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        members = soup.find_all(\"li\", class_=\"compact\")\n",
    "\n",
    "        for member in members:\n",
    "            # Scrape the information\n",
    "            items = member.find_all(\"span\", class_=\"result-item\")\n",
    "            name = sanitize_name(member.span.a.text)\n",
    "            \n",
    "            for item in items:\n",
    "                if item.strong.text == \"Party:\":\n",
    "                    affiliation = item.span.text\n",
    "\n",
    "            congress_members.append((name, affiliation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "len(congress_members) == 1158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "to_csv(\"politicians_congress.csv\", politicians)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually might not be useful (and less of a headache) to just take the congress list, since all politicians from the github list are elected (meaning they are or were congress members)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.2 - Extract quotes from politicians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded another list, from the congress [website](https://bioguide.congress.gov/search?index=%22bioguideprofiles%22&size=12&matches=[]&filters={%22jobPositions.congressAffiliation.partyAffiliation.party.name%22:[%22Democrat%22,%22Republican%22],%22jobPositions.congressAffiliation.congress.name%22:[%22The%20110th%20United%20States%20Congress%22,%22The%20111th%20United%20States%20Congress%22,%22The%20112th%20United%20States%20Congress%22,%22The%20113th%20United%20States%20Congress%22,%22The%20114th%20United%20States%20Congress%22,%22The%20115th%20United%20States%20Congress%22,%22The%20116th%20United%20States%20Congress%22,%22The%20117th%20United%20States%20Congress%22]}&sort=[{%22_score%22:true},{%22field%22:%22familyName%22,%22order%22:%22asc%22},{%22field%22:%22middleName%22,%22order%22:%22asc%22},{%22field%22:%22givenName%22,%22order%22:%22asc%22}])\n",
    "\n",
    "\n",
    "Those are the politicians from 2007 to 2009. The json is called `data/resources/congress_biolist.json`. What is nice is that we have the \"congress bio ID\" of each congress member, which is also present in the `speaker_attributes.parquet` file (field `US_congress_bio_ID`). We will use that to extract the tweets from the politicians.\n",
    "\n",
    "Testing with quotes from 2008."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63902/3013702173.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"quotes-2008.json.bz2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100_000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_63902/127175552.py\u001b[0m in \u001b[0;36mload_df\u001b[0;34m(file_name, mode, save, chunksize)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mdf_lst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mdf_reader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                 \u001b[0mdf_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/courses/ADA/ada-2021-project-apa/venv/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    796\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    797\u001b[0m             \u001b[0mlines_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_combine_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 798\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_object_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m             \u001b[0;31m# Make sure that the returned objects have the right index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/courses/ADA/ada-2021-project-apa/venv/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_get_object_parser\u001b[0;34m(self, json)\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"frame\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 770\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFrameParser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    771\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"series\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/courses/ADA/ada-2021-project-apa/venv/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/courses/ADA/ada-2021-project-apa/venv/lib/python3.9/site-packages/pandas/io/json/_json.py\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprecise_float\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m             )\n\u001b[1;32m   1142\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df2 = load_df(\"quotes-2008.json.bz2\", chunksize=100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_63902/1830384054.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "len(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_filepath = os.path.join(RESOURCES_PATH, \"congress_biolist.json\")\n",
    "quotes_filepath = os.path.join(DATA_PATH, \"quotes-2008.json.bz2\")\n",
    "# keys = ['quoteID', 'quotation', 'speaker', 'date', 'numOccurrences', 'phase']\n",
    "\n",
    "politicians_df = pd.read_json(politicians_filepath).drop(\"congresses\", axis=1)\n",
    "quotes_df = pd.read_pickle(os.path.join(PKL_PATH, \"quotes-2008.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lst = []\n",
    "with pd.read_json(quotes_filepath, lines=True, chunksize=2000) as df_reader:\n",
    "    for i, chunk in enumerate(df_reader):\n",
    "        data_lst.append(chunk)\n",
    "        if i == 1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4000 entries, 0 to 3999\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype         \n",
      "---  ------          --------------  -----         \n",
      " 0   quoteID         4000 non-null   object        \n",
      " 1   quotation       4000 non-null   object        \n",
      " 2   speaker         4000 non-null   object        \n",
      " 3   qids            4000 non-null   object        \n",
      " 4   date            4000 non-null   datetime64[ns]\n",
      " 5   numOccurrences  4000 non-null   int64         \n",
      " 6   probas          4000 non-null   object        \n",
      " 7   urls            4000 non-null   object        \n",
      " 8   phase           4000 non-null   object        \n",
      "dtypes: datetime64[ns](1), int64(1), object(7)\n",
      "memory usage: 281.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.concat(data_lst)\n",
    "len(data_df)\n",
    "data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4641330 entries, 0 to 4641329\n",
      "Data columns (total 6 columns):\n",
      " #   Column          Dtype \n",
      "---  ------          ----- \n",
      " 0   quoteID         object\n",
      " 1   quotation       object\n",
      " 2   speaker         object\n",
      " 3   date            object\n",
      " 4   numOccurrences  int64 \n",
      " 5   phase           object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 212.5+ MB\n"
     ]
    }
   ],
   "source": [
    "quotes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "232066"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just taking a subsample of the quotes dataset to check if everything is working in a timely manner\n",
    "quotes_df = quotes_df.sample(int(np.floor(0.05 * len(quotes_df))))\n",
    "len(quotes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quoteID</th>\n",
       "      <th>quotation</th>\n",
       "      <th>speaker</th>\n",
       "      <th>date</th>\n",
       "      <th>numOccurrences</th>\n",
       "      <th>phase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4043163</th>\n",
       "      <td>2008-11-07-048453</td>\n",
       "      <td>there are 670 helicopters in new zealand and 8...</td>\n",
       "      <td>john sinclair</td>\n",
       "      <td>2008-11-07 00:57:33</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1610076</th>\n",
       "      <td>2008-12-02-024980</td>\n",
       "      <td>if they fail to do so, there is nowhere else t...</td>\n",
       "      <td>sonny perdue</td>\n",
       "      <td>2008-12-02 17:40:16</td>\n",
       "      <td>3</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3420914</th>\n",
       "      <td>2008-09-04-060729</td>\n",
       "      <td>ugh, he's just writing whatever he feels like</td>\n",
       "      <td>dave eggers</td>\n",
       "      <td>2008-09-04 01:38:30</td>\n",
       "      <td>2</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926354</th>\n",
       "      <td>2008-09-10-074668</td>\n",
       "      <td>with a mission to help individuals objectively...</td>\n",
       "      <td>david fleming</td>\n",
       "      <td>2008-09-10 12:56:41</td>\n",
       "      <td>1</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2875163</th>\n",
       "      <td>2008-09-29-060554</td>\n",
       "      <td>we are conducting our own testing and will nee...</td>\n",
       "      <td>daniel ellis</td>\n",
       "      <td>2008-09-29 19:34:00</td>\n",
       "      <td>6</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   quoteID                                          quotation  \\\n",
       "4043163  2008-11-07-048453  there are 670 helicopters in new zealand and 8...   \n",
       "1610076  2008-12-02-024980  if they fail to do so, there is nowhere else t...   \n",
       "3420914  2008-09-04-060729      ugh, he's just writing whatever he feels like   \n",
       "926354   2008-09-10-074668  with a mission to help individuals objectively...   \n",
       "2875163  2008-09-29-060554  we are conducting our own testing and will nee...   \n",
       "\n",
       "               speaker                 date  numOccurrences phase  \n",
       "4043163  john sinclair  2008-11-07 00:57:33               1     A  \n",
       "1610076   sonny perdue  2008-12-02 17:40:16               3     A  \n",
       "3420914    dave eggers  2008-09-04 01:38:30               2     A  \n",
       "926354   david fleming  2008-09-10 12:56:41               1     A  \n",
       "2875163   daniel ellis  2008-09-29 19:34:00               6     A  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "quotes_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>givenName</th>\n",
       "      <th>familyName</th>\n",
       "      <th>unaccentedGivenName</th>\n",
       "      <th>unaccentedFamilyName</th>\n",
       "      <th>birthYear</th>\n",
       "      <th>deathYear</th>\n",
       "      <th>middleName</th>\n",
       "      <th>unaccentedMiddleName</th>\n",
       "      <th>nickName</th>\n",
       "      <th>honorificPrefix</th>\n",
       "      <th>honorificSuffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A000014</td>\n",
       "      <td>Neil</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>Neil</td>\n",
       "      <td>Abercrombie</td>\n",
       "      <td>1938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A000374</td>\n",
       "      <td>Ralph</td>\n",
       "      <td>Abraham</td>\n",
       "      <td>Ralph</td>\n",
       "      <td>Abraham</td>\n",
       "      <td>1954</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A000022</td>\n",
       "      <td>Gary</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>Gary</td>\n",
       "      <td>Ackerman</td>\n",
       "      <td>1942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>Leonard</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A000370</td>\n",
       "      <td>Alma</td>\n",
       "      <td>Adams</td>\n",
       "      <td>Alma</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A000366</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>Adams</td>\n",
       "      <td>Sandra</td>\n",
       "      <td>Adams</td>\n",
       "      <td>1956</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sandy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id givenName   familyName unaccentedGivenName unaccentedFamilyName  \\\n",
       "0  A000014      Neil  Abercrombie                Neil          Abercrombie   \n",
       "1  A000374     Ralph      Abraham               Ralph              Abraham   \n",
       "2  A000022      Gary     Ackerman                Gary             Ackerman   \n",
       "3  A000370      Alma        Adams                Alma                Adams   \n",
       "4  A000366    Sandra        Adams              Sandra                Adams   \n",
       "\n",
       "   birthYear  deathYear middleName unaccentedMiddleName nickName  \\\n",
       "0       1938        NaN        NaN                  NaN      NaN   \n",
       "1       1954        NaN        NaN                  NaN      NaN   \n",
       "2       1942        NaN    Leonard              Leonard      NaN   \n",
       "3       1946        NaN        NaN                  NaN      NaN   \n",
       "4       1956        NaN        NaN                  NaN    Sandy   \n",
       "\n",
       "  honorificPrefix honorificSuffix  \n",
       "0             NaN             NaN  \n",
       "1             NaN             NaN  \n",
       "2             NaN             NaN  \n",
       "3             NaN             NaN  \n",
       "4             NaN             NaN  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "politicians_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "politicians_df[\"fullName\"] = politicians_df[\"givenName\"] + \" \" + politicians_df[\"familyName\"]\n",
    "politicians_df[\"fullName\"] = politicians_df[\"fullName\"].str.lower()\n",
    "names = politicians_df[\"fullName\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46413/46413 [00:35<00:00, 1304.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# about 55min on my shitty laptop (for the whole 4millions dataset)\n",
    "# 10% subset takes about 5min\n",
    "quotes_df[\"subset\"] = quotes_df[\"speaker\"].progress_apply(lambda x: pd.Series(x).isin(names))\n",
    "quotes_df[\"subset\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46413/46413 [00:49<00:00, 943.37it/s] \n"
     ]
    }
   ],
   "source": [
    "# about 1h20min on my shitty laptop\n",
    "# 10% subset takes about 5min\n",
    "# -> preferred method if we want to check for \"republicans\" or \"democrats\", slower otherwise\n",
    "quotes_df[\"subset\"] = quotes_df[\"speaker\"].progress_apply(lambda x: pd.Series(x).str.contains(\"|\".join(names), case=False))\n",
    "quotes_df[\"subset\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "congress_members = politicians_df[\"fullName\"].tolist()  # redefined here for clarity\n",
    "\n",
    "def extract_subset(orig_df: pd.DataFrame, multiproc=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function extracts the quotes of speakers that are in the congress list.\n",
    "\n",
    "    It returns the number of extracted quotes and the extracted dataframe.\n",
    "    \"\"\"\n",
    "    if multiproc:\n",
    "        orig_df[\"subset\"] = orig_df[\"speaker\"].apply_parallel(lambda x: pd.Series(x).isin(names))\n",
    "    else:\n",
    "        orig_df[\"subset\"] = orig_df[\"speaker\"].progress_apply(lambda x: pd.Series(x).isin(names))\n",
    "\n",
    "    return orig_df[\"subset\"].sum(), orig_df[orig_df[\"subset\"] == True]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 232066/232066 [03:06<00:00, 1245.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# Without multiprocessing\n",
    "# 3m05s on 5% of the 2008 dataset (~230k quotes)\n",
    "subset_count, subset_2008 = extract_subset(quotes_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiprocessing for extraction\n",
    "# 50s on 5% of the 2008 dataset (~230k quotes)\n",
    "subset_count, subset_2008 = extract_subset(quotes_df, multiproc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data/pkl/quotes-2008.pkl',\n",
       " 'data/pkl/quotes-2009.pkl',\n",
       " 'data/pkl/quotes-2010.pkl',\n",
       " 'data/pkl/quotes-2011.pkl',\n",
       " 'data/pkl/quotes-2012.pkl',\n",
       " 'data/pkl/quotes-2013.pkl',\n",
       " 'data/pkl/quotes-2014.pkl',\n",
       " 'data/pkl/quotes-2015.pkl',\n",
       " 'data/pkl/quotes-2016.pkl',\n",
       " 'data/pkl/quotes-2017.pkl',\n",
       " 'data/pkl/quotes-2018.pkl',\n",
       " 'data/pkl/quotes-2019.pkl',\n",
       " 'data/pkl/quotes-2020.pkl']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Goal is to do a pipeline to automatically extract quotes for a quotes dataset\n",
    "# The datasets were already loaded from the json.bz2 format and converted to .pkl in `data/pkl`\n",
    "\n",
    "# Get the names\n",
    "quotes_datasets = [os.path.join(\"data\", \"pkl\", f\"quotes-20{i:02d}.pkl\") for i in range(8, 21)]  \n",
    "\n",
    "# For each dataset, extract the quotes from congress members\n",
    "# and save the extracted quotes as pkl for easier handling\n",
    "for i, dataset in enumerate(quotes_datasets, start=1):\n",
    "    print(f\"{i}/{len(quotes_datasets)} {dataset}:\")\n",
    "    try:\n",
    "        complete_df = pd.read_pickle(dataset)  # Load dataset\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{dataset} not found, loading from .json.bz2\")\n",
    "        complete_df = load_df(dataset)\n",
    "\n",
    "    _, subset_df = extract_subset(complete_df)\n",
    "    subset_df.to_pickle(os.path.join(\"data\", \"pkl\", f\"extracted_{dataset}\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/13 data/pkl/quotes-2008.pkl:\n",
      "2/13 data/pkl/quotes-2009.pkl:\n",
      "data/pkl/quotes-2009.pkl not found, loading from .json.bz2\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'DATA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21247/2478367864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mcomplete_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/courses/ADA/ada-2021-project-apa/venv/lib/python3.9/site-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/projects/courses/ADA/ada-2021-project-apa/venv/lib/python3.9/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/pkl/quotes-2009.pkl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21247/2478367864.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{dataset} not found, loading from .json.bz2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mcomplete_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_21247/3876349852.py\u001b[0m in \u001b[0;36mload_df\u001b[0;34m(file_name, mode, save)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"bz2\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "quotes_datasets = [os.path.join(\"data\", \"pkl\", f\"quotes-20{i:02d}.pkl\") for i in range(8, 21)]  \n",
    "\n",
    "for i, dataset in enumerate(quotes_datasets, start=1):\n",
    "    print(f\"{i}/{len(quotes_datasets)} {dataset}:\")\n",
    "    try:\n",
    "        print(\"Reading from pkl\")\n",
    "        complete_df = pd.read_pickle(dataset)  # Load dataset\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{dataset} not found, loading from .json.bz2\")\n",
    "        complete_df = load_df(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6afe1dc6d4d3cd304b95f6d675ef2d0511325e35f0283dd46abd30ad807ae0f6"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
