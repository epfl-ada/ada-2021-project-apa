{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf093eba-c6fe-4390-a57f-efee79da586e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data extraction\n",
    "\n",
    "This notebook describes the data extraction steps that were undertaken to get our final dataset. Approaches that were tried but ended up _not_ being used are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbc25a-a8f4-41c7-898e-5bbca37bd412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "476725bc-31a1-4be4-a8b4-3a70fb217ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import json\n",
    "import bz2\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Third parties\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4e51bc-e051-4b16-99dc-2b3dda281030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization needed for some modules\n",
    "\n",
    "# tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f4c234-ac9a-4124-a21c-87fc301e6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"data\"\n",
    "PKL_PATH = os.path.join(DATA_PATH, \"pkl\")\n",
    "CSV_PATH = os.path.join(DATA_PATH, \"csv\")\n",
    "RESOURCES_PATH = os.path.join(DATA_PATH, \"resources\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0449d4-c122-45cc-a9c7-95eb60883343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "\n",
    "def to_csv(file_name: str, pol_lst: list) -> None:\n",
    "    \"\"\"\n",
    "    Write list to csv.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_path = os.path.join(RESOURCES_PATH, file_name)\n",
    "\n",
    "    with open(csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=\" \")\n",
    "        writer.writerow([\"Name\", \"Party\"])\n",
    "\n",
    "        for member in pol_lst:\n",
    "            writer.writerow([el for el in member])\n",
    "            \n",
    "def get_pkl_year(year: int) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of the pkl files present in `PKL_PATH/{year}`.\n",
    "    \"\"\"\n",
    "\n",
    "    dirs = os.listdir(os.path.join(PKL_PATH, str(year)))\n",
    "\n",
    "    return [os.path.join(str(year), dir) for dir in dirs]\n",
    "\n",
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip and clean name.\n",
    "    \"Senator Cruz, Ted\" -> \"Ted Cruz\"\n",
    "    \"\"\"\n",
    "\n",
    "    for element in (\"Representative\", \"Senator\"):\n",
    "        name = name.strip(element)\n",
    "\n",
    "    name = \" \".join(name.split(\",\")[::-1])\n",
    "    name = name.strip()\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510bedb-3a64-4a6c-bb98-3db561a63a69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Get list of US politicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cf068-717f-4ee3-b08d-37e7d2529f88",
   "metadata": {},
   "source": [
    "First of all, we need to have a list of current US politicians to be able to extract their quotes. After some research, we found two possible candidates:\n",
    "- the official [US congress website](https://www.congress.gov/members?q={%22congress%22:[%22110%22,%22111%22,%22112%22,%22113%22,%22114%22,%22115%22,%22116%22,117]})\n",
    "- a list of US politicians extracted from Twitter for a [research](https://github.com/casmlab/politicians-tweets). The politicians list is available on [Github](https://raw.githubusercontent.com/casmlab/politicians-tweets/main/metadata/usa/current.json). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36a877-81d9-4b55-94e7-0d522d85a49a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### US congress website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f93674-3c85-4471-9a2f-0b6ec806bcef",
   "metadata": {},
   "source": [
    "Since it is an official source, it should be reliable. The caveat is that no official API exists, so the content needs to be scraped. That is what is done in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb45120f-5aed-4de0-86f5-f26e09db25b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:09<00:00,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "URL = 'https://www.congress.gov/members?q={\"congress\":[\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",117]}&pageSize=250'\n",
    "congress_members = []\n",
    "\n",
    "# Download each congress page\n",
    "with requests.Session() as s:\n",
    "    for page_number in tqdm(range(1, 6)):\n",
    "        r  = s.get(URL, params={\"page\": page_number})\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        members = soup.find_all(\"li\", class_=\"compact\")\n",
    "\n",
    "        for member in members:\n",
    "            # Scrape the information\n",
    "            items = member.find_all(\"span\", class_=\"result-item\")\n",
    "            name = sanitize_name(member.span.a.text)\n",
    "            \n",
    "            for item in items:\n",
    "                if item.strong.text == \"Party:\":\n",
    "                    affiliation = item.span.text\n",
    "\n",
    "            congress_members.append((name, affiliation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7ba0c4-bd37-4e5a-8578-405e78be380e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of retrieved congress members 1160\n"
     ]
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(f\"Number of retrieved congress members {len(congress_members)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5d3024-05ae-45fe-abf6-c93822ea02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "to_csv(\"politicians_congress.csv\", congress_members)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87169a0-3dea-4aab-92fb-63f15a1fbc2a",
   "metadata": {},
   "source": [
    "So we have 1160 congress members. This is satisfactory to start with. One caveat is that we only have the politician's name and party affiliation. We do not easily have more information (age, state, gender, ...) without matching the data with some other dataset (wikidata for example). This would mean additional work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17839ebc-a674-43e3-a5fa-55bde35910d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Github list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9717ccf-41de-49ee-ac5c-e9aaa5d1acef",
   "metadata": {},
   "source": [
    "This research [1] \"collect tweets posted by politicians in the U.S. and India and save the JSON provided by the Twitter API. Lists of politicians are generated by NivaDuck, software developed at Microsoft Research - India for automatically identifying accounts that belong to politicians\". \n",
    "\n",
    "In this section, we explore the given list to see if we could use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0412820f-3017-476c-acf2-34ce3c61e9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Json columns dict_keys(['id', 'id_str', 'screen_name', 'confirmed_account_type', 'state', 'twitter_name', 'real_name', 'bioguide', 'office_holder', 'party', 'district', 'level', 'woman', 'birthday', 'last_updated'])\n"
     ]
    }
   ],
   "source": [
    "# Load the json file\n",
    "file_name = \"politicians_github.json\"\n",
    "file_path = os.path.join(RESOURCES_PATH, file_name)\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    json = json.load(f)\n",
    "    \n",
    "# Print the keys to see what we have\n",
    "print(f\"Json columns {json.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a29fdf-0510-4b7e-bb93-1a93c2bea241",
   "metadata": {},
   "source": [
    "As written in their research, they selected all \"politicians\" classified as such by the NivaDuck software. This means that some entries are not professional politicians (ie. congress members), but just influential people. We decide to only keep the official politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b3ba3f8-e2c2-4d08-b93e-cc61968c54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1107 in the Github dataset\n"
     ]
    }
   ],
   "source": [
    "# Only keep politicians with political affiliation\n",
    "politicians = []\n",
    "\n",
    "for i in range(1, len(json[\"id\"])):\n",
    "    i = str(i)  # index is a string in json\n",
    "    affiliation = json[\"party\"][i]\n",
    "    screen_name = json[\"screen_name\"][i]\n",
    "    elected = json[\"office_holder\"][i] is not None\n",
    "\n",
    "    if affiliation is not None and affiliation in (\"Republican\", \"Democratic\"):\n",
    "        politicians.append((json[\"real_name\"][i], affiliation, elected))\n",
    "    elif screen_name == \"realdonaldtrump\":\n",
    "        politicians.append((\"Donald Trump\", \"Republican\", True))\n",
    "    elif screen_name == \"barackobama\":\n",
    "        politicians.append((\"Barack Obama\", \"Democratic\", True))\n",
    "        \n",
    "# Count how many politicians are \"elected\" (-> congress members)\n",
    "elected_count = sum(pol[-1] for pol in politicians)\n",
    "print(f\"{elected_count} in the Github dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a521b6-1455-41ea-ae4d-66ca6a4e335c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(politicians)=1107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Mark Green', 'Republican', True),\n",
       " ('Pete Stauber', 'Republican', True),\n",
       " ('Derek Kilmer', 'Democratic', True),\n",
       " ('Andy Harris', 'Republican', True),\n",
       " ('Donald Payne', 'Democratic', True)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sanity check\n",
    "print(f\"{len(politicians)=}\") \n",
    "politicians[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8b7fe-9019-4ec6-8a1b-75de83bd092b",
   "metadata": {},
   "source": [
    "In this dataset, 1107 congress members (and Trump) are present. This is less than the 1158 from the official congress website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a588dd8-507b-496e-b7be-e63564cff440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "to_csv(\"politicians_github.csv\", politicians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585f59f-fb5c-4822-aff2-39eb53c13fa8",
   "metadata": {},
   "source": [
    "[1] Panda, A., Gonawela, A., Acharyya, S., Mishra, D., Mohapatra, M., Chandrasekaran, R., & Pal, J. (2020). NivaDuck - A Scalable Pipeline to Build a Database of Political Twitter Handles for India and the United States. International Conference on Social Media and Society, 200–209. https://doi.org/10.1145/3400806.3400830"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa63535-c5d6-4250-9e55-869f1f5e9c5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Final chosen politicians list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d39ab-5164-472b-9063-d8497c739919",
   "metadata": {},
   "source": [
    "Since we have more politicians in the official congress' list, we should keep that one. As already explained, one caveat is that we don't have much information apart from the politician's name. \n",
    "\n",
    "However, we are lucky. An official list of congress members with plenty information does exist! In fact, it was even mentioned on the project's page, what a shame that we did not see it sooner (_sigh_). \n",
    "\n",
    "This resource is the official biography list of the congress: [congress list](https://bioguide.congress.gov/search?index=%22bioguideprofiles%22&size=12&matches=%5B%5D&filters=%7B%22jobPositions.congressAffiliation.partyAffiliation.party.name%22:%5B%22Democrat%22,%22Republican%22%5D,%22jobPositions.congressAffiliation.congress.name%22:%5B%22The%20110th%20United%20States%20Congress%22,%22The%20111th%20United%20States%20Congress%22,%22The%20112th%20United%20States%20Congress%22,%22The%20113th%20United%20States%20Congress%22,%22The%20114th%20United%20States%20Congress%22,%22The%20115th%20United%20States%20Congress%22,%22The%20116th%20United%20States%20Congress%22,%22The%20117th%20United%20States%20Congress%22%5D%7D&sort=%5B%7B%22_score%22:true%7D,%7B%22field%22:%22familyName%22,%22order%22:%22asc%22%7D,%7B%22field%22:%22middleName%22,%22order%22:%22asc%22%7D,%7B%22field%22:%22givenName%22,%22order%22:%22asc%22%7D%5D). We select the congress members from 2007 up to today. \n",
    "\n",
    "The data can be directly exported as `json`. Also we have access to the \"congress bio ID\" of each congress member, which is also present in the `speaker_attributes.parquet` file (`field US_congress_bio_ID`). Even if we don't use that directly, we can use that information later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "886e9b60-3880-4ad3-b939-846c7e969459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list\n",
    "politicians_filepath = os.path.join(RESOURCES_PATH, \"congress_biolist.json\")\n",
    "politicians_df = pd.read_json(politicians_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b545a6-ca41-42e7-8053-eddb265323f5",
   "metadata": {},
   "source": [
    "One issue that was detected (which also happens for the first official congress list) is that Donald Trump is not in the dataset (because he was President, not senator or representative and thus, not a congress member). As he is expected to be the `speaker` of many quotes, we will need to manually add him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95b68233-5d5f-4667-8bb3-de79a3e80c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually add Donald Trump\n",
    "# Not so elegant trick to capture variations of the name\n",
    "# Should refactore to another solution (alias field of speaker_attributes) later\n",
    "\n",
    "donald_json1 = {\n",
    "    \"id\": np.nan,\n",
    "    \"givenName\": \"Donald\",\n",
    "    \"familyName\": \"Trump\",\n",
    "    \"unaccentedGivenName\": \"Donald\",\n",
    "    \"unaccentedFamilyName\": \"Trump\",\n",
    "    \"birthYear\": 1946,\n",
    "    \"deathYear\": np.nan,\n",
    "    \"congresses\": [\n",
    "        {\n",
    "            \"position\": \"President\",\n",
    "            \"congressNumber\": np.nan,\n",
    "            \"stateName\": np.nan,\n",
    "            \"parties\": [\"Republican\"],\n",
    "        }\n",
    "    ],\n",
    "    \"middleName\": \"John\",\n",
    "    \"unaccentedMiddleName\": \"John\",\n",
    "    \"nickName\": np.nan,\n",
    "    \"honorificPrefix\": np.nan,\n",
    "    \"honorificSuffix\": np.nan,\n",
    "}\n",
    "\n",
    "donald_json2 = donald_json1.copy()\n",
    "donald_json2[\"givenName\"] = \"President\"\n",
    "donald_json2[\"unaccentedGivenName\"] = \"President\"\n",
    "\n",
    "donald_json3 = donald_json1.copy()\n",
    "donald_json3[\"givenName\"] = \"President Donald\"\n",
    "donald_json3[\"unaccentedGivenName\"] = \"President Donald\"\n",
    "\n",
    "politicians_df = politicians_df.append(\n",
    "    pd.DataFrame([donald_json1, donald_json2, donald_json3]), ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82b4da3c-2389-4f3e-9842-c9715eff126c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>givenName</th>\n",
       "      <th>familyName</th>\n",
       "      <th>unaccentedGivenName</th>\n",
       "      <th>unaccentedFamilyName</th>\n",
       "      <th>birthYear</th>\n",
       "      <th>deathYear</th>\n",
       "      <th>congresses</th>\n",
       "      <th>middleName</th>\n",
       "      <th>unaccentedMiddleName</th>\n",
       "      <th>nickName</th>\n",
       "      <th>honorificPrefix</th>\n",
       "      <th>honorificSuffix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>Z000018</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>Zinke</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>Zinke</td>\n",
       "      <td>1961</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'position': 'Representative', 'congressNumbe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1155</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'position': 'President', 'congressNumber': n...</td>\n",
       "      <td>John</td>\n",
       "      <td>John</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1156</th>\n",
       "      <td>NaN</td>\n",
       "      <td>President</td>\n",
       "      <td>Trump</td>\n",
       "      <td>President</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'position': 'President', 'congressNumber': n...</td>\n",
       "      <td>John</td>\n",
       "      <td>John</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>NaN</td>\n",
       "      <td>President Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>President Donald</td>\n",
       "      <td>Trump</td>\n",
       "      <td>1946</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'position': 'President', 'congressNumber': n...</td>\n",
       "      <td>John</td>\n",
       "      <td>John</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id         givenName familyName unaccentedGivenName  \\\n",
       "1154  Z000018              Ryan      Zinke                Ryan   \n",
       "1155      NaN            Donald      Trump              Donald   \n",
       "1156      NaN         President      Trump           President   \n",
       "1157      NaN  President Donald      Trump    President Donald   \n",
       "\n",
       "     unaccentedFamilyName  birthYear  deathYear  \\\n",
       "1154                Zinke       1961        NaN   \n",
       "1155                Trump       1946        NaN   \n",
       "1156                Trump       1946        NaN   \n",
       "1157                Trump       1946        NaN   \n",
       "\n",
       "                                             congresses middleName  \\\n",
       "1154  [{'position': 'Representative', 'congressNumbe...        NaN   \n",
       "1155  [{'position': 'President', 'congressNumber': n...       John   \n",
       "1156  [{'position': 'President', 'congressNumber': n...       John   \n",
       "1157  [{'position': 'President', 'congressNumber': n...       John   \n",
       "\n",
       "     unaccentedMiddleName nickName honorificPrefix honorificSuffix  \n",
       "1154                  NaN      NaN             NaN             NaN  \n",
       "1155                 John      NaN             NaN             NaN  \n",
       "1156                 John      NaN             NaN             NaN  \n",
       "1157                 John      NaN             NaN             NaN  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that Trump was indeed appended to the df\n",
    "politicians_df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77d8c505-69ea-4808-9f8f-cc9e27e5c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export new df to json\n",
    "politicians_df.to_json(os.path.join(RESOURCES_PATH, \"new_congress_biolist.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f59fef6-0e2b-4dee-bd44-6d9fb8b32158",
   "metadata": {},
   "source": [
    "This dirty trick allows us to capture quotes from Donald Trump and its most appearing variations (\"Donald Trump\", \"President Trump\", \"President Donald Trump\"). The complex structure of the json is there to match the initial congress biography list.\n",
    "\n",
    "As mentioned in the comment, a more elegant solution can (and will) be implemented using the `speaker_attributes` `alias` field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d932be3-1855-4902-b565-12e62d7a93b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Extract quotes from politicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460aab01-6a15-4b41-9a82-3af3f7cc928b",
   "metadata": {},
   "source": [
    "Since we now have our list of politicians, we need to extract their quotes from the Quotebank dataset (quotation centric).\n",
    "To do that, and to split the load between team members, we decided to first find a way to load the quotebank dataset of a given year into pandas, which is not possible to do in one chunk because of memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdd77f0e-aa62-4d05-befe-983be13c07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed for the task\n",
    "\n",
    "def load_df(\n",
    "    file_name: str, mode: str = \"pandas\", save: bool = True, chunksize: int = 500_000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a dataset in DataFrame from a .json.bz2 archive.\n",
    "\n",
    "    file_name: str\n",
    "        Name of .json.bz2 archive to load from `DATA_PATH`.\n",
    "\n",
    "    mode: str = \"pandas\" | \"bz2\"\n",
    "        Either use pandas read_json function or homemade bz2 function. This is usually faster (but makes my computer crash for some reason).\n",
    "        Mode \"bz2\" should be used if you are sure that the dataframe can fit into memory.\n",
    "\n",
    "    save: bool\n",
    "        Save the dataframe as a pickle file in `PKL_PATH`.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(DATA_PATH, file_name)\n",
    "\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(\"File not found\")\n",
    "        return\n",
    "    \n",
    "    # Extract yer from file_name\n",
    "    year_re = r\"20\\d\\d\"\n",
    "    year_file = re.search(year_re, file_name).group(0)\n",
    "\n",
    "    if mode == \"bz2\":  # Only use if can fit in memory!\n",
    "        if save:\n",
    "            # Be sure to inform the user that we are not saving\n",
    "            # even though flag is set\n",
    "            print(\"Save option currently not supported for \\\"bz2\\\" mode.\")\n",
    "        \n",
    "        # Subset of keys to load\n",
    "        keys = [\"quoteID\", \"quotation\", \"speaker\", \"date\", \"numOccurrences\", \"phase\"]  \n",
    "\n",
    "        with bz2.open(file_path, \"rb\") as quote_file:\n",
    "            df = pd.DataFrame(\n",
    "                [\n",
    "                    dict(zip(keys, map(json.loads(instance).get, keys)))\n",
    "                    for instance in tqdm(quote_file)\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "            \n",
    "    else:  # pandas load\n",
    "        if not save:  # force the need to save \n",
    "            print(\"Please enable save option.\")\n",
    "            return\n",
    "        \n",
    "        # Load in chunks and save to pickle\n",
    "        with pd.read_json(file_path, lines=True, chunksize=chunksize) as df_reader:\n",
    "            for i, chunk in enumerate(df_reader):\n",
    "                file_name = file_name.strip(\".json.bz2\")\n",
    "                pkl_path = os.path.join(PKL_PATH, year_file, f\"{file_name}-{i:03d}.pkl\")\n",
    "                chunk.to_pickle(pkl_path)\n",
    "    \n",
    "        # If we use pandas, we only return the last chunk (for debugging)\n",
    "        return chunk\n",
    "\n",
    "def extract_subset(orig_df: pd.DataFrame, multiproc=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function extracts the quotes of speakers that are in the congress list.\n",
    "    It returns the number of extracted quotes and the extracted dataframe.\n",
    "    \n",
    "    Multiprocessing is supported. Set to False if any issue is encountered.\n",
    "    \"\"\"\n",
    "\n",
    "    if multiproc:\n",
    "        # Load module and initialize only if we need it\n",
    "        from pandarallel import pandarallel\n",
    "        pandarallel.initialize(progress_bar=True)\n",
    "        \n",
    "        orig_df[\"subset\"] = orig_df[\"speaker\"].parallel_apply(\n",
    "            lambda x: pd.Series(x.lower()).str.contains(\"|\".join(congress_members))\n",
    "        )\n",
    "    else:\n",
    "        orig_df[\"subset\"] = orig_df[\"speaker\"].progress_apply(\n",
    "            lambda x: pd.Series(x.lower()).str.contains(\"|\".join(congress_members))\n",
    "        )\n",
    "\n",
    "    return orig_df[\"subset\"].sum(), orig_df[orig_df[\"subset\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6f8f6-aceb-46c0-9505-31904fa056fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Do not run this cell, it takes (many) hours.\n",
    "# It was run once with this exact code to generate the working dataframes.\n",
    "##\n",
    "\n",
    "# Load each .json.bz2 archive, load it in chunks, convert to pd.DataFrame and save to pickle\n",
    "\n",
    "archives = [f\"quotes-20{i:02d}.json.bz2\" for i in range(15, 21)]  \n",
    "# The following code is disabled such that this heavy code does not run if someone runs the cell on accident. \n",
    "# Change False to True to enable the code\n",
    "if False:\n",
    "    for i, archive in enumerate(archives, start=1):\n",
    "        print(f\"{i}/{len(archives)} {archive}:\")\n",
    "        try:\n",
    "            load_df(archive)\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{archive} not found, going to next file\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e63f1-c8d9-479b-9c1e-82a71b7afd98",
   "metadata": {},
   "source": [
    "Now that we have access to chunks of the total dataset of each year, we can simply loop through each chunk to extract the relevant quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e6225c5-3bd4-4983-94e0-5532aa59ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column from politicians to match the speaker field from quotebank\n",
    "politicians_df[\"fullName\"] = politicians_df[\"givenName\"] + \" \" + politicians_df[\"familyName\"]\n",
    "politicians_df[\"fullName\"] = politicians_df[\"fullName\"].str.lower()\n",
    "\n",
    "congress_members = politicians_df[\"fullName\"].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d535c48-a771-413f-9f84-b28177bdceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Also computation-heavy cell. \n",
    "##\n",
    "\n",
    "# The datasets were already loaded from the json.bz2 format and converted to .pkl in in chunks in `data/pkl/{year}`\n",
    "\n",
    "# Loop through each year of interest\n",
    "for year_i in range(2015, 2021):\n",
    "    print(year_i)\n",
    "    \n",
    "    # Get all the chunks for the given year\n",
    "    files = get_pkl_year(year_i)\n",
    "\n",
    "    # Extract the quotes of interest of each chunk\n",
    "    all_extracted = []\n",
    "    for file in files:\n",
    "        df = pd.read_pickle(os.path.join(PKL_PATH, file))\n",
    "        _, subset_df = extract_subset(df)\n",
    "        all_extracted.append(subset_df)\n",
    "\n",
    "    # Merge them into a new df\n",
    "    df_extracted = pd.concat(all_extracted)\n",
    "    \n",
    "    # Print sanity check\n",
    "    print(f\"{len(df_extracted)=}\")\n",
    "\n",
    "    # Save the df as pkl\n",
    "    pkl_name = f\"extracted-quotes-{year_i}.pkl\"\n",
    "    df_extracted.to_pickle(os.path.join(PKL_PATH, pkl_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fc8f5-58d8-4b70-9de9-5c1ed247f5f8",
   "metadata": {},
   "source": [
    "Nice! We manage to handle the huge size of the data and we are getting to some manageable size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b65cd-cbc5-4718-bf14-7fb6b1cdea03",
   "metadata": {},
   "source": [
    "## 3. Extract mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6750af3-24d3-4d7b-82dd-09d1fb027269",
   "metadata": {},
   "source": [
    "The last thing that we need to do is to extract the quotes of politicians that are mentioning a policitian from the other party, eg. a republican mentioning a democrat, or inversely. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91d16ec-95f4-45a0-a5a8-dcf0489dea47",
   "metadata": {},
   "source": [
    "### Work on politicians_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a815dd2",
   "metadata": {},
   "source": [
    "First, the data that we need from the politicians list is in a \"raw\" format. We want to have it in a \"clean\" way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "629079a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1158/1158 [00:03<00:00, 361.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Extract from congresses column relevant infor (position, state, parties)\n",
    "def extract_congress_information(row):\n",
    "    information = pd.json_normalize(row[\"congresses\"]).sort_values(\"congressNumber\").tail(1).loc[:, [\"position\",\"stateName\",\"parties\"]]\n",
    "\n",
    "    row[\"position\"] = information.loc[:, \"position\"].values[0]\n",
    "    row[\"stateName\"] = information.loc[:, \"stateName\"].values[0]\n",
    "    row[\"parties\"] = information.loc[:, \"parties\"].values[0]\n",
    "    return row\n",
    "\n",
    "politicians_df = politicians_df.progress_apply(extract_congress_information, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0ffa376c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The value in column parties is a list, we want to select the last past party from the list\n",
    "def getLastValue(aList):\n",
    "    return aList[-1]\n",
    "\n",
    "politicians_df[\"parties\"] = politicians_df[\"parties\"].apply(getLastValue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe18e9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column speaker (full name of the politician)\n",
    "politicians_df['speaker'] = politicians_df['givenName'] + \" \" + politicians_df['familyName']\n",
    "\n",
    "# Have the speaker's full names in the same size \n",
    "politicians_df[\"speaker\"] = politicians_df[\"speaker\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "44536fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "timothy johnson           2\n",
       "donald payne              2\n",
       "duncan hunter             2\n",
       "patrick murphy            2\n",
       "gregory murphy            1\n",
       "                         ..\n",
       "ruben gallego             1\n",
       "pete gallego              1\n",
       "elton gallegly            1\n",
       "michael gallagher         1\n",
       "president donald trump    1\n",
       "Name: speaker, Length: 1154, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for full name duplicates\n",
    "politicians_df['speaker'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f01c88e",
   "metadata": {},
   "source": [
    "The above cell refers to the remark in the README file. For now, we decide to remove those \"duplicates\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bd36a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We drop duplicates by speaker's full name\n",
    "politicians_df = politicians_df.drop_duplicates(subset=['speaker'], keep=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7789fb",
   "metadata": {},
   "source": [
    "### Merge and select subset\n",
    "\n",
    "Then, we want to merge the quotes dataset of a given year with the politicians dataset and then directly select quotes that are mentioning politicians from the other camp. Below is the whole pipeline to do that.\n",
    "\n",
    "That code is proposed for a single year. We ran it for every year of interest to extract the relevant quotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "312586a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = \"2015\"\n",
    "\n",
    "# Load the main dataset\n",
    "df = pd.read_pickle(os.path.join(PKL_PATH, f\"extracted-quotes-{YEAR}.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c79705cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(subset_democrats)=166494\n",
      "len(subset_republicans)=253528\n"
     ]
    }
   ],
   "source": [
    "# Set speaker's name to lowercase\n",
    "df[\"speaker\"] = df[\"speaker\"].str.lower()\n",
    "\n",
    "# Merge quotes to speaker's info\n",
    "data = pd.merge(df, politicians_df, on='speaker', how='outer')\n",
    "\n",
    "# Subsets by parties \n",
    "subset_democrats = data[data['parties'] == \"Democrat\"]\n",
    "subset_republicans = data[data['parties'] == \"Republican\"]\n",
    "\n",
    "# Sanity checks\n",
    "print(f\"{len(subset_democrats)=}\")\n",
    "print(f\"{len(subset_republicans)=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3981d4",
   "metadata": {},
   "source": [
    "Then, out of those subsets, we want to extract only the quotes that are mentioning a politician from the other camp. For now, we are limiting ourselves to a \"naive\" approach, where we only detect when the full name (given name + family name in lowercase) is mentioned. Also note that we added the detection of \"republican(s)\" or \"democrat(s)\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "250f0c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of Republicans and Democrats\n",
    "names_democrats = politicians_df[politicians_df['parties'] == 'Democrat'][\"speaker\"].tolist()\n",
    "names_republicans = politicians_df[politicians_df['parties'] == 'Republican'][\"speaker\"].tolist()\n",
    "\n",
    "# Ensure that all quotes are in lowercase\n",
    "subset_democrats.loc[:, \"quotation\"] = subset_democrats[\"quotation\"].str.lower()\n",
    "subset_republicans.loc[:, \"quotation\"] = subset_republicans[\"quotation\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a497e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with those who don't have any quotes (no speaker detected)\n",
    "subset_democrats = subset_democrats.dropna(subset=['quotation'])\n",
    "subset_republicans = subset_republicans.dropna(subset=['quotation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0c86c88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create full lists that we want to match against\n",
    "list_rep = names_republicans + ['republicans?']\n",
    "pattern_list_rep = '|'.join(list_rep)\n",
    "\n",
    "list_dem = names_democrats + ['democrats?']\n",
    "pattern_list_dem = '|'.join(list_dem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "16b142ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual extraction\n",
    "# Subset of quotes said by democrats about republicans\n",
    "# This cell takes ~1min to complete\n",
    "demo_quotes_abt_rep = subset_democrats[subset_democrats['quotation'].str.contains(pattern_list_rep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3d705ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of quotes said by democrats about democrats\n",
    "# This cell takes ~1min to complete\n",
    "rep_quotes_abt_demo = subset_republicans[subset_republicans['quotation'].str.contains(pattern_list_dem)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7690a4",
   "metadata": {},
   "source": [
    "We simply then save the extracted quotes for each year to csv. The csv format was preferred for this task because we encountered some issues when using pickle files on different machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "94a9aeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to CSV\n",
    "df_to_save = pd.concat([demo_quotes_abt_rep, rep_quotes_abt_demo])\n",
    "file_path = os.path.join(CSV_PATH, f\"{YEAR}_mentions.csv\")\n",
    "df_to_save.to_csv(file_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "edc1a23f945431ce4c0ce12a191555bee6d7f0d3bb9235137bba8922e6ad584d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
