{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf093eba-c6fe-4390-a57f-efee79da586e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Data extraction\n",
    "\n",
    "This notebook describes the data extraction steps that were undertaken to get our final dataset. Approaches that were tried but ended up _not_ being used are also included."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbc25a-a8f4-41c7-898e-5bbca37bd412",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476725bc-31a1-4be4-a8b4-3a70fb217ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Built-in\n",
    "import json\n",
    "import bz2\n",
    "import os\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "\n",
    "# Third parties\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from pandarallel import pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4e51bc-e051-4b16-99dc-2b3dda281030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization needed for some modules\n",
    "\n",
    "# tqdm for pandas\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f4c234-ac9a-4124-a21c-87fc301e6003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "DATA_PATH = \"data\"\n",
    "PKL_PATH = os.path.join(DATA_PATH, \"pkl\")\n",
    "CSV_PATH = os.path.join(DATA_PATH, \"csv\")\n",
    "RESOURCES_PATH = os.path.join(DATA_PATH, \"resources\")\n",
    "YEAR = \"2015\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0449d4-c122-45cc-a9c7-95eb60883343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "\n",
    "def to_csv(file_name: str, pol_lst: list) -> None:\n",
    "    \"\"\"\n",
    "    Write list to csv.\n",
    "    \"\"\"\n",
    "\n",
    "    csv_path = os.path.join(RESOURCES_PATH, file_name)\n",
    "\n",
    "    with open(csv_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f, delimiter=\" \")\n",
    "        writer.writerow([\"Name\", \"Party\"])\n",
    "\n",
    "        for member in pol_lst:\n",
    "            writer.writerow([el for el in member])\n",
    "            \n",
    "def get_pkl_year(year: int) -> list:\n",
    "    \"\"\"\n",
    "    Returns a list of the pkl files present in `PKL_PATH/{year}`.\n",
    "    \"\"\"\n",
    "\n",
    "    dirs = os.listdir(os.path.join(PKL_PATH, str(year)))\n",
    "\n",
    "    return [os.path.join(str(year), dir) for dir in dirs]\n",
    "\n",
    "def sanitize_name(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Strip and clean name.\n",
    "    \"Senator Cruz, Ted\" -> \"Ted Cruz\"\n",
    "    \"\"\"\n",
    "\n",
    "    for element in (\"Representative\", \"Senator\"):\n",
    "        name = name.strip(element)\n",
    "\n",
    "    name = \" \".join(name.split(\",\")[::-1])\n",
    "    name = name.strip()\n",
    "    \n",
    "    return name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e510bedb-3a64-4a6c-bb98-3db561a63a69",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 1. Get list of US politicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10cf068-717f-4ee3-b08d-37e7d2529f88",
   "metadata": {},
   "source": [
    "First of all, we need to have a list of current US politicians to be able to extract their quotes. After some research, we found two possible candidates:\n",
    "- the official [US congress website](https://www.congress.gov/members?q={%22congress%22:[%22110%22,%22111%22,%22112%22,%22113%22,%22114%22,%22115%22,%22116%22,117]})\n",
    "- a list of US politicians extracted from Twitter for a [research](https://github.com/casmlab/politicians-tweets). The politicians list is available on [Github](https://raw.githubusercontent.com/casmlab/politicians-tweets/main/metadata/usa/current.json). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f36a877-81d9-4b55-94e7-0d522d85a49a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### US congress website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f93674-3c85-4471-9a2f-0b6ec806bcef",
   "metadata": {},
   "source": [
    "Since it is an official source, it should be reliable. The caveat is that no official API exists, so the content needs to be scraped. That is what is done in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb45120f-5aed-4de0-86f5-f26e09db25b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.congress.gov/members?q={\"congress\":[\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",117]}&pageSize=250'\n",
    "congress_members = []\n",
    "\n",
    "# Download each congress page\n",
    "with requests.Session() as s:\n",
    "    for page_number in tqdm(range(1, 6)):\n",
    "        r  = s.get(URL, params={\"page\": page_number})\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        members = soup.find_all(\"li\", class_=\"compact\")\n",
    "\n",
    "        for member in members:\n",
    "            # Scrape the information\n",
    "            items = member.find_all(\"span\", class_=\"result-item\")\n",
    "            name = sanitize_name(member.span.a.text)\n",
    "            \n",
    "            for item in items:\n",
    "                if item.strong.text == \"Party:\":\n",
    "                    affiliation = item.span.text\n",
    "\n",
    "            congress_members.append((name, affiliation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7ba0c4-bd37-4e5a-8578-405e78be380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(f\"Number of retrieved congress members {len(congress_members)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d3024-05ae-45fe-abf6-c93822ea02f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "to_csv(\"politicians_congress.csv\", politicians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87169a0-3dea-4aab-92fb-63f15a1fbc2a",
   "metadata": {},
   "source": [
    "So we have 1158 congress members. This is satisfactory to start with. One caveat is that we only have the politician's name and party affiliation. We do not easily have more information (age, state, gender, ...) without matching the data with some other dataset (wikidata for example). This would mean additional work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17839ebc-a674-43e3-a5fa-55bde35910d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Github list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9717ccf-41de-49ee-ac5c-e9aaa5d1acef",
   "metadata": {},
   "source": [
    "This research [1] \"collect tweets posted by politicians in the U.S. and India and save the JSON provided by the Twitter API. Lists of politicians are generated by NivaDuck, software developed at Microsoft Research - India for automatically identifying accounts that belong to politicians\". \n",
    "\n",
    "In this section, we explore the given list to see if we could use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0412820f-3017-476c-acf2-34ce3c61e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the json file\n",
    "file_name = \"politicians_github.json\"\n",
    "file_path = os.path.join(RESOURCES_PATH, file_name)\n",
    "\n",
    "with open(file_path, \"r\") as f:\n",
    "    json = json.load(f)\n",
    "    \n",
    "# Print the keys to see what we have\n",
    "print(f\"Json columns {json.keys()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a29fdf-0510-4b7e-bb93-1a93c2bea241",
   "metadata": {},
   "source": [
    "As written in their research, they selected all \"politicians\" classified as such by the NivaDuck software. This means that some entries are not professional politicians (ie. congress members), but just influential people. We decide to only keep the official politicians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ba3f8-e2c2-4d08-b93e-cc61968c54a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep politicians with political affiliation\n",
    "politicians = []\n",
    "\n",
    "for i in tqdm(range(1, len(json[\"id\"]))):\n",
    "    i = str(i)  # index is a string in json\n",
    "    affiliation = json[\"party\"][i]\n",
    "    screen_name = json[\"screen_name\"][i]\n",
    "    elected = json[\"office_holder\"][i] is not None\n",
    "\n",
    "    if affiliation is not None and affiliation in (\"Republican\", \"Democratic\"):\n",
    "        politicians.append((json[\"real_name\"][i], affiliation, elected))\n",
    "    elif screen_name == \"realdonaldtrump\":\n",
    "        politicians.append((\"Donald Trump\", \"Republican\", True))\n",
    "    elif screen_name == \"barackobama\":\n",
    "        politicians.append((\"Barack Obama\", \"Democratic\", True))\n",
    "        \n",
    "# Count how many politicians are \"elected\" (-> congress members)\n",
    "elected_count = sum(pol[-1] for pol in politicians)\n",
    "print(f\"{elected_count} in the Github dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a521b6-1455-41ea-ae4d-66ca6a4e335c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check\n",
    "print(f\"{len(politicians)=}\") \n",
    "politicians[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed8b7fe-9019-4ec6-8a1b-75de83bd092b",
   "metadata": {},
   "source": [
    "In this dataset, 1107 congress members (and Trump) are present. This is less than the 1158 from the official congress website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a588dd8-507b-496e-b7be-e63564cff440",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file\n",
    "to_csv(\"politicians_github.csv\", politicians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9585f59f-fb5c-4822-aff2-39eb53c13fa8",
   "metadata": {},
   "source": [
    "[1] Panda, A., Gonawela, A., Acharyya, S., Mishra, D., Mohapatra, M., Chandrasekaran, R., & Pal, J. (2020). NivaDuck - A Scalable Pipeline to Build a Database of Political Twitter Handles for India and the United States. International Conference on Social Media and Society, 200â€“209. https://doi.org/10.1145/3400806.3400830"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa63535-c5d6-4250-9e55-869f1f5e9c5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Final chosen politicians list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032d39ab-5164-472b-9063-d8497c739919",
   "metadata": {},
   "source": [
    "Since we have more politicians in the official congress' list, we should keep that one. As already explained, one caveat is that we don't have much information apart from the politician's name. \n",
    "\n",
    "However, we are lucky. An official list of congress members with plenty information does exist! In fact, it was even mentioned on the project's page, what a shame that we did not see it sooner (_sigh_). \n",
    "\n",
    "This resource is the official biography list of the congress: [congress list](https://bioguide.congress.gov/search?index=%22bioguideprofiles%22&size=12&matches=%5B%5D&filters=%7B%22jobPositions.congressAffiliation.partyAffiliation.party.name%22:%5B%22Democrat%22,%22Republican%22%5D,%22jobPositions.congressAffiliation.congress.name%22:%5B%22The%20110th%20United%20States%20Congress%22,%22The%20111th%20United%20States%20Congress%22,%22The%20112th%20United%20States%20Congress%22,%22The%20113th%20United%20States%20Congress%22,%22The%20114th%20United%20States%20Congress%22,%22The%20115th%20United%20States%20Congress%22,%22The%20116th%20United%20States%20Congress%22,%22The%20117th%20United%20States%20Congress%22%5D%7D&sort=%5B%7B%22_score%22:true%7D,%7B%22field%22:%22familyName%22,%22order%22:%22asc%22%7D,%7B%22field%22:%22middleName%22,%22order%22:%22asc%22%7D,%7B%22field%22:%22givenName%22,%22order%22:%22asc%22%7D%5D). We select the congress members from 2007 up to today. \n",
    "\n",
    "The data can be directly exported as `json`. Also we have access to the \"congress bio ID\" of each congress member, which is also present in the `speaker_attributes.parquet` file (`field US_congress_bio_ID`). Even if we don't use that directly, we can use that information later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886e9b60-3880-4ad3-b939-846c7e969459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the list\n",
    "politicians_filepath = os.path.join(RESOURCES_PATH, \"congress_biolist.json\")\n",
    "politicians_df = pd.read_json(politicians_filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fe371-e2c9-4957-b39c-da31cf9dff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check some information\n",
    "print(f\"Columns: {politicians_df.columns}\")\n",
    "print(f\"Length: {len(politicians_df)}\")\n",
    "politicians_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b545a6-ca41-42e7-8053-eddb265323f5",
   "metadata": {},
   "source": [
    "One issue that was detected (which also happens for the first official congress list) is that Donald Trump is not in the dataset (because he was President, not senator or representative and thus, not a congress member). As he is expected to be the `speaker` of many quotes, we will need to manually add him."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b68233-5d5f-4667-8bb3-de79a3e80c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually add Donald Trump\n",
    "# Not so elegant trick to capture variations of the name\n",
    "# Should refactore to another solution (alias field of speaker_attributes) later\n",
    "\n",
    "donald_json1 = {\n",
    "    \"id\": np.nan,\n",
    "    \"givenName\": \"Donald\",\n",
    "    \"familyName\": \"Trump\",\n",
    "    \"unaccentedGivenName\": \"Donald\",\n",
    "    \"unaccentedFamilyName\": \"Trump\",\n",
    "    \"birthYear\": 1946,\n",
    "    \"deathYear\": np.nan,\n",
    "    \"congresses\": [\n",
    "        {\n",
    "            \"position\": \"President\",\n",
    "            \"congressNumber\": np.nan,\n",
    "            \"stateName\": np.nan,\n",
    "            \"parties\": [\"Republican\"],\n",
    "        }\n",
    "    ],\n",
    "    \"middleName\": \"John\",\n",
    "    \"unaccentedMiddleName\": \"John\",\n",
    "    \"nickName\": np.nan,\n",
    "    \"honorificPrefix\": np.nan,\n",
    "    \"honorificSuffix\": np.nan,\n",
    "}\n",
    "\n",
    "donald_json2 = {\n",
    "    \"id\": np.nan,\n",
    "    \"givenName\": \"President\",\n",
    "    \"familyName\": \"Trump\",\n",
    "    \"unaccentedGivenName\": \"President\",\n",
    "    \"unaccentedFamilyName\": \"Trump\",\n",
    "    \"birthYear\": 1946,\n",
    "    \"deathYear\": np.nan,\n",
    "    \"congresses\": [\n",
    "        {\n",
    "            \"position\": \"President\",\n",
    "            \"congressNumber\": np.nan,\n",
    "            \"stateName\": np.nan,\n",
    "            \"parties\": [\"Republican\"],\n",
    "        }\n",
    "    ],\n",
    "    \"middleName\": \"John\",\n",
    "    \"unaccentedMiddleName\": \"John\",\n",
    "    \"nickName\": np.nan,\n",
    "    \"honorificPrefix\": np.nan,\n",
    "    \"honorificSuffix\": np.nan,\n",
    "}\n",
    "\n",
    "donald_json3 = {\n",
    "    \"id\": np.nan,\n",
    "    \"givenName\": \"President Donald\",\n",
    "    \"familyName\": \"Trump\",\n",
    "    \"unaccentedGivenName\": \"President Donald\",\n",
    "    \"unaccentedFamilyName\": \"Trump\",\n",
    "    \"birthYear\": 1946,\n",
    "    \"deathYear\": np.nan,\n",
    "    \"congresses\": [\n",
    "        {\n",
    "            \"position\": \"President\",\n",
    "            \"congressNumber\": np.nan,\n",
    "            \"stateName\": np.nan,\n",
    "            \"parties\": [\"Republican\"],\n",
    "        }\n",
    "    ],\n",
    "    \"middleName\": \"John\",\n",
    "    \"unaccentedMiddleName\": \"John\",\n",
    "    \"nickName\": np.nan,\n",
    "    \"honorificPrefix\": np.nan,\n",
    "    \"honorificSuffix\": np.nan,\n",
    "}\n",
    "\n",
    "politicians_df = politicians_df.append(\n",
    "    pd.DataFrame([donald_json1, donald_json2, donald_json3]), ignore_index=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b4da3c-2389-4f3e-9842-c9715eff126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that Trump was indeed appended to the df\n",
    "politicians_df.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d8c505-69ea-4808-9f8f-cc9e27e5c4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export new df to json\n",
    "politicians_df.to_json(os.path.join(RESOURCES_PATH, \"new_congress_biolist.json\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f59fef6-0e2b-4dee-bd44-6d9fb8b32158",
   "metadata": {},
   "source": [
    "This dirty trick allows us to capture quotes from Donald Trump and its most appearing variations (\"Donald Trump\", \"President Trump\", \"President Donald Trump\"). The complex structure of the json is there to match the initial congress biography list.\n",
    "\n",
    "As mentioned in the comment, a more elegant solution can (and will) be implemented using the `speaker_attributes` `alias` field. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d932be3-1855-4902-b565-12e62d7a93b1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## 2. Extract quotes from politicians"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460aab01-6a15-4b41-9a82-3af3f7cc928b",
   "metadata": {},
   "source": [
    "Since we now have our list of politicians, we need to extract their quotes from the Quotebank dataset (quotation centric).\n",
    "To do that, and to split the load between team members, we decided to first find a way to load the quotebank dataset of a given year into pandas, which is not possible to do in one chunk because of memory limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd77f0e-aa62-4d05-befe-983be13c07c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions needed for the task\n",
    "\n",
    "def load_df(\n",
    "    file_name: str, mode: str = \"pandas\", save: bool = True, chunksize: int = 500_000\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load a dataset in DataFrame from a .json.bz2 archive.\n",
    "\n",
    "    file_name: str\n",
    "        Name of .json.bz2 archive to load from `DATA_PATH`.\n",
    "\n",
    "    mode: str = \"pandas\" | \"bz2\"\n",
    "        Either use pandas read_json function or homemade bz2 function. This is usually faster (but makes my computer crash for some reason).\n",
    "        Mode \"bz2\" should be used if you are sure that the dataframe can fit into memory.\n",
    "\n",
    "    save: bool\n",
    "        Save the dataframe as a pickle file in `PKL_PATH`.\n",
    "    \"\"\"\n",
    "\n",
    "    file_path = os.path.join(DATA_PATH, file_name)\n",
    "    \n",
    "    # Extract yer from file_name\n",
    "    year_re = r\"20\\d\\d\"\n",
    "    year_file = re.search(year_re, file_name).group(0)\n",
    "\n",
    "    if mode == \"bz2\":  # Only use if can fit in memory!\n",
    "        if not save:\n",
    "            # Be sure to inform the user that we are not saving\n",
    "            # even though flag is set\n",
    "            print(\"Save option currently not supported for \\\"bz2\\\" mode.\")\n",
    "            return\n",
    "        \n",
    "        # Subset of keys to load\n",
    "        keys = [\"quoteID\", \"quotation\", \"speaker\", \"date\", \"numOccurrences\", \"phase\"]  \n",
    "\n",
    "        with bz2.open(file_path, \"rb\") as quote_file:\n",
    "            df = pd.DataFrame(\n",
    "                [\n",
    "                    dict(zip(keys, map(json.loads(instance).get, keys)))\n",
    "                    for instance in tqdm(quote_file)\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "        return df\n",
    "            \n",
    "    else:  # pandas load\n",
    "        if not save:  # force the need to save \n",
    "            print(\"Please enable save option.\")\n",
    "            return\n",
    "        \n",
    "        # Load in chunks and save to pickle\n",
    "        with pd.read_json(file_path, lines=True, chunksize=chunksize) as df_reader:\n",
    "            for i, chunk in enumerate(df_reader):\n",
    "                file_name = file_name.strip(\".json.bz2\")\n",
    "                pkl_path = os.path.join(PKL_PATH, year_file, f\"{file_name}-{i:03d}.pkl\")\n",
    "                chunk.to_pickle(pkl_path)\n",
    "    \n",
    "        # If we use pandas, we only return the last chunk (for debugging)\n",
    "        return chunk\n",
    "\n",
    "def extract_subset(orig_df: pd.DataFrame, multiproc=False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    This function extracts the quotes of speakers that are in the congress list.\n",
    "    It returns the number of extracted quotes and the extracted dataframe.\n",
    "    \n",
    "    Multiprocessing is supported. Set to False if any issue is encountered.\n",
    "    \"\"\"\n",
    "\n",
    "    if multiproc:\n",
    "        # Load module and initialize\n",
    "        from pandarallel import pandarallel\n",
    "        pandarallel.initialize(progress_bar=True)\n",
    "        \n",
    "        orig_df[\"subset\"] = orig_df[\"speaker\"].parallel_apply(\n",
    "            lambda x: pd.Series(x.lower()).str.contains(\"|\".join(congress_members))\n",
    "        )\n",
    "    else:\n",
    "        orig_df[\"subset\"] = orig_df[\"speaker\"].progress_apply(\n",
    "            lambda x: pd.Series(x.lower()).str.contains(\"|\".join(congress_members))\n",
    "        )\n",
    "\n",
    "    return orig_df[\"subset\"].sum(), orig_df[orig_df[\"subset\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6f8f6-aceb-46c0-9505-31904fa056fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Do not run this cell, it takes (many) hours.\n",
    "# It was run once with this exact code to generate the working dataframes.\n",
    "##\n",
    "\n",
    "# Load each .json.bz2 archive, load it in chunks, convert to pd.DataFrame and save to pickle\n",
    "\n",
    "archives = [os.path.join(DATA_PATH, f\"quotes-20{i:02d}.json.bz2\") for i in range(15, 21)]  \n",
    "\n",
    "for i, archive in enumerate(archives, start=1):\n",
    "    print(f\"{i}/{len(archives)} {archive}:\")\n",
    "    try:\n",
    "        load_df(archive, multiproc=True)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"{archive} not found, going to next file\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e63f1-c8d9-479b-9c1e-82a71b7afd98",
   "metadata": {},
   "source": [
    "Now that we have access to chunks of the total dataset of each year, we can simply loop through each chunk to extract the relevant quotes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6225c5-3bd4-4983-94e0-5532aa59ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column from politicians to match the speaker field from quotebank\n",
    "politicians_df[\"fullName\"] = politicians_df[\"givenName\"] + \" \" + politicians_df[\"familyName\"]\n",
    "politicians_df[\"fullName\"] = politicians_df[\"fullName\"].str.lower()\n",
    "\n",
    "congress_members = politicians_df[\"fullName\"].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d535c48-a771-413f-9f84-b28177bdceb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Also computation-heavy cell. \n",
    "##\n",
    "\n",
    "# The datasets were already loaded from the json.bz2 format and converted to .pkl in in chunks in `data/pkl/{year}`\n",
    "\n",
    "# Loop through each year of interest\n",
    "for year_i in range(2015, 2021):\n",
    "    print(year_i)\n",
    "    \n",
    "    # Get all the chunks for the given year\n",
    "    files = get_pkl_year(year_i)\n",
    "\n",
    "    # Extract the quotes of interest of each chunk\n",
    "    all_extracted = []\n",
    "    for file in files:\n",
    "        df = pd.read_pickle(os.path.join(PKL_PATH, file))\n",
    "        _, subset_df = extract_subset(df)\n",
    "        all_extracted.append(subset_df)\n",
    "\n",
    "    # Merge them into a new df\n",
    "    df_extracted = pd.concat(all_extracted)\n",
    "    \n",
    "    # Print sanity check\n",
    "    print(f\"{len(df_extracted=)}\")\n",
    "\n",
    "    # Save the df as pkl\n",
    "    pkl_name = f\"extracted-quotes-{year_i}.pkl\"\n",
    "    df_extracted.to_pickle(os.path.join(PKL_PATH, pkl_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6fc8f5-58d8-4b70-9de9-5c1ed247f5f8",
   "metadata": {},
   "source": [
    "Nice! We manage to handle the huge size of the data and we are getting to some manageable size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b65cd-cbc5-4718-bf14-7fb6b1cdea03",
   "metadata": {},
   "source": [
    "## 3. Extract mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6750af3-24d3-4d7b-82dd-09d1fb027269",
   "metadata": {},
   "source": [
    "The last thing that we need to do is to extract the quotes of politicians that are mentioning a policitian from the other party, eg. a republican mentioning a democrat, or inversely. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b91d16ec-95f4-45a0-a5a8-dcf0489dea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO, from \"mentions_extraction.ipynb\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('venv': venv)",
   "language": "python",
   "name": "python395jvsc74a57bd06afe1dc6d4d3cd304b95f6d675ef2d0511325e35f0283dd46abd30ad807ae0f6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
